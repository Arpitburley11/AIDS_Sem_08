{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ARPIT S. BURLEY                                                          \n",
        "D16AD                                                                    \n",
        "09                                    "
      ],
      "metadata": {
        "id": "i2so5hiKEq2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Initialize environment with a small state space for simplicity\n",
        "num_states = 5   # Example with 5 states (0 to 4)\n",
        "num_actions = 2  # Two possible actions (e.g., left or right)\n",
        "\n",
        "# Q-tables for Double Q-Learning (two separate tables to reduce maximization bias)\n",
        "Q1 = np.zeros((num_states, num_actions))\n",
        "Q2 = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate: How much new information overrides the old\n",
        "gamma = 0.9  # Discount factor: Importance of future rewards\n",
        "epsilon = 0.1  # Exploration probability: Chance of choosing a random action (for exploration)\n",
        "\n",
        "def choose_action(state):\n",
        "    \"\"\"\n",
        "    Choose an action based on epsilon-greedy strategy:\n",
        "    - With probability epsilon, choose a random action (exploration)\n",
        "    - Otherwise, choose the action with the highest estimated value from Q1 + Q2 (exploitation)\n",
        "    \"\"\"\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(range(num_actions))  # Explore (random action)\n",
        "    else:\n",
        "        return np.argmax(Q1[state] + Q2[state])  # Exploit (best estimated action)\n",
        "\n",
        "# Simulating learning over episodes\n",
        "for episode in range(1000):\n",
        "    state = random.randint(0, num_states - 1)  # Start at a random state\n",
        "    next_state = (state + 1) % num_states  # Example transition: Move to the next state in a circular manner\n",
        "    reward = 1 if next_state == num_states - 1 else 0  # Reward for reaching the last state\n",
        "\n",
        "    action = choose_action(state)  # Choose an action based on Q-values and epsilon-greedy strategy\n",
        "\n",
        "    # Randomly decide which Q-table to update (reduces overestimation bias)\n",
        "    if random.random() < 0.5:\n",
        "        best_next_action = np.argmax(Q1[next_state])  # Choose best action from Q1 for next state\n",
        "        Q1[state, action] += alpha * (reward + gamma * Q2[next_state, best_next_action] - Q1[state, action])\n",
        "    else:\n",
        "        best_next_action = np.argmax(Q2[next_state])  # Choose best action from Q2 for next state\n",
        "        Q2[state, action] += alpha * (reward + gamma * Q1[next_state, best_next_action] - Q2[state, action])\n",
        "\n",
        "# Print the learned Q-values for both tables and their average estimate\n",
        "print(\"Q1 Table:\\n\", Q1)\n",
        "print(\"Q2 Table:\\n\", Q2)\n",
        "print(\"Final Q-Value Estimate (Averaged):\\n\", (Q1 + Q2) / 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OHT4kMTLKcD",
        "outputId": "d0ef69f3-5a63-49d7-c768-59db3251336c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1 Table:\n",
            " [[0.99934521 0.12009005]\n",
            " [1.18318636 0.15225754]\n",
            " [1.44406667 0.18812757]\n",
            " [1.67918487 0.38666038]\n",
            " [0.85136662 0.14297449]]\n",
            "Q2 Table:\n",
            " [[1.01088778 0.44127548]\n",
            " [1.2021054  0.39335903]\n",
            " [1.42142338 0.2310557 ]\n",
            " [1.69756098 0.19762396]\n",
            " [0.83322835 0.17433988]]\n",
            "Final Q-Value Estimate (Averaged):\n",
            " [[1.0051165  0.28068276]\n",
            " [1.19264588 0.27280829]\n",
            " [1.43274502 0.20959164]\n",
            " [1.68837293 0.29214217]\n",
            " [0.84229748 0.15865719]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define a restaurant rating environment\n",
        "num_states = 5  # Example: 5 different customer satisfaction levels\n",
        "num_actions = 3  # Actions: (0) Improve food, (1) Improve service, (2) Improve ambiance)\n",
        "\n",
        "# Q-tables for Double Q-Learning\n",
        "Q1 = np.zeros((num_states, num_actions))\n",
        "Q2 = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration probability\n",
        "\n",
        "def choose_action(state):\n",
        "    \"\"\"\n",
        "    Choose an action based on epsilon-greedy strategy:\n",
        "    - With probability epsilon, choose a random action (exploration)\n",
        "    - Otherwise, choose the action with the highest estimated value from Q1 + Q2 (exploitation)\n",
        "    \"\"\"\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(range(num_actions))  # Explore (random action)\n",
        "    else:\n",
        "        return np.argmax(Q1[state] + Q2[state])  # Exploit (best estimated action)\n",
        "\n",
        "# Simulating learning over episodes\n",
        "for episode in range(1000):\n",
        "    state = random.randint(0, num_states - 1)  # Start at a random customer satisfaction level\n",
        "    action = choose_action(state)  # Choose an action (food, service, or ambiance improvement)\n",
        "\n",
        "    # Define next state and reward logic\n",
        "    if action == 0:  # Improve food\n",
        "        next_state = min(state + 1, num_states - 1)  # Food improvement increases satisfaction\n",
        "    elif action == 1:  # Improve service\n",
        "        next_state = min(state + 2, num_states - 1)  # Service improvement gives bigger boost\n",
        "    else:  # Improve ambiance\n",
        "        next_state = max(state - 1, 0)  # Ambiance change might not always be positive\n",
        "\n",
        "    reward = next_state  # Higher satisfaction states give better rewards\n",
        "\n",
        "    # Randomly decide which Q-table to update\n",
        "    if random.random() < 0.5:\n",
        "        best_next_action = np.argmax(Q1[next_state])\n",
        "        Q1[state, action] += alpha * (reward + gamma * Q2[next_state, best_next_action] - Q1[state, action])\n",
        "    else:\n",
        "        best_next_action = np.argmax(Q2[next_state])\n",
        "        Q2[state, action] += alpha * (reward + gamma * Q1[next_state, best_next_action] - Q2[state, action])\n",
        "\n",
        "# Print the learned Q-values for both tables and their average estimate\n",
        "print(\"\\n--- Final Learned Q-Values ---\")\n",
        "print(\"\\nQ1 Table (Action-Value Estimates from Q1):\\n\", Q1)\n",
        "print(\"\\nQ2 Table (Action-Value Estimates from Q2):\\n\", Q2)\n",
        "print(\"\\nFinal Averaged Q-Values (Best Policy Estimate):\\n\", (Q1 + Q2) / 2)\n",
        "print(\"\\nLegend:\")\n",
        "print(\"- Row index represents customer satisfaction levels (0 = Very Dissatisfied, 4 = Very Satisfied)\")\n",
        "print(\"- Columns represent actions: 0 = Improve Food, 1 = Improve Service, 2 = Improve Ambiance\")\n",
        "print(\"- Higher values indicate better long-term action choices for increasing customer satisfaction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6YzuStSLsHv",
        "outputId": "2f4d0f13-d3e4-4dc3-951d-a88f3c6cc925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Learned Q-Values ---\n",
            "\n",
            "Q1 Table (Action-Value Estimates from Q1):\n",
            " [[18.77712542  3.05158369  4.2272931 ]\n",
            " [21.75682471  7.63658646  2.39810902]\n",
            " [24.23429419  7.5455421   1.25692048]\n",
            " [24.62552034  4.11215675  2.00764103]\n",
            " [24.59466828  8.28032217  5.16736455]]\n",
            "\n",
            "Q2 Table (Action-Value Estimates from Q2):\n",
            " [[19.415341    7.17767576  0.93932024]\n",
            " [21.61876277  6.138231    0.        ]\n",
            " [23.46245133  8.08255041  2.17283571]\n",
            " [24.84413542  3.14933171  5.27710152]\n",
            " [24.43733785  8.51632715  1.54229379]]\n",
            "\n",
            "Final Averaged Q-Values (Best Policy Estimate):\n",
            " [[19.09623321  5.11462972  2.58330667]\n",
            " [21.68779374  6.88740873  1.19905451]\n",
            " [23.84837276  7.81404626  1.71487809]\n",
            " [24.73482788  3.63074423  3.64237127]\n",
            " [24.51600307  8.39832466  3.35482917]]\n",
            "\n",
            "Legend:\n",
            "- Row index represents customer satisfaction levels (0 = Very Dissatisfied, 4 = Very Satisfied)\n",
            "- Columns represent actions: 0 = Improve Food, 1 = Improve Service, 2 = Improve Ambiance\n",
            "- Higher values indicate better long-term action choices for increasing customer satisfaction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bDk9VTQTAO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}